# -*- coding: utf-8 -*-
"""Credit Risk Default Modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ruCp-ixSMgvfPzHoWLOF2X9z6iPzcn0T
"""

import pandas as pd
import numpy as np

import math
import seaborn as sns


from matplotlib.pyplot import subplots
import itertools
import time
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.api import OLS
import sklearn. model_selection as skm
import sklearn. linear_model as skl
from sklearn. preprocessing import StandardScaler
from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error
!pip install ISLP
from ISLP import load_data
from ISLP.models import ModelSpec as MS
from functools import partial
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split

from sklearn. pipeline import Pipeline
from sklearn. decomposition import PCA
from sklearn. cross_decomposition import PLSRegression
from ISLP.models import \
(Stepwise ,
sklearn_selected ,
sklearn_selection_path )
!pip install l0bnb
from l0bnb import fit_path

"""#Q5 - In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.
#Q5(a) -  Fit a logistic regression model that uses income and balance to predict default.
"""

default_data = load_data('Default')
default = pd.DataFrame(default_data)
default['student'] = default['student'].map({'No': 0, 'Yes': 1})
default['default'] = default['default'].map({'No': 0, 'Yes': 1})
default.head()

allvars = default.columns.drop (['default', 'student'])
design = MS(allvars)
x = design. fit_transform (default)
y = default['default'] == 'Yes'

glm = sm.GLM(y,
             x,
             family=sm. families.Binomial())
results = glm.fit()
results.summary()

"""#5(b) -Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:
#i. Split the sample set into a training set and a validation set.
#ii. Fit a multiple logistic regression model using only the train- ing observations.
#iii. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.
#iv. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.
"""

default_data = load_data('Default')
default_train , default_valid = train_test_split (default_data,
test_size =5000 ,
random_state =0)

default_mm = MS(['balance', 'income'])
x_train = default_mm.fit_transform (default_train)
y_train = default_train['default'].map({'No': 0, 'Yes': 1})
model = sm.OLS(y_train , x_train)
results = model.fit()

x_valid = default_mm.transform(default_valid )
y_valid = default_valid['default'].map({'No': 0, 'Yes': 1})
valid_probs = results.predict(x_valid)
valid_pred = (valid_probs > 0.5).astype(int)
np.mean(y_valid == valid_pred)

validation_set_error = 1 - np.mean(y_valid == valid_pred)
print("Validation Set Error:", validation_set_error)

"""#5(c) - Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained."""

default_data = load_data('Default')
default_train , default_valid = train_test_split (default_data,
test_size = 2500 ,
random_state =0)
default_mm = MS(['balance', 'income'])
x_train = default_mm.fit_transform (default_train)
y_train = default_train['default'].map({'No': 0, 'Yes': 1})
model = sm.OLS(y_train , x_train)
results = model.fit()
x_valid = default_mm.transform(default_valid )
y_valid = default_valid['default'].map({'No': 0, 'Yes': 1})
valid_probs = results.predict(x_valid)
valid_pred = (valid_probs > 0.5).astype(int)
np.mean(y_valid == valid_pred)
validation_set_error = 1 - np.mean(y_valid == valid_pred)
print("Validation Set Error:", validation_set_error)

default_data = load_data('Default')
default_train , default_valid = train_test_split (default_data,
test_size = 7500 ,
random_state =0)
default_mm = MS(['balance', 'income'])
x_train = default_mm.fit_transform (default_train)
y_train = default_train['default'].map({'No': 0, 'Yes': 1})
model = sm.OLS(y_train , x_train)
results = model.fit()
x_valid = default_mm.transform(default_valid )
y_valid = default_valid['default'].map({'No': 0, 'Yes': 1})
valid_probs = results.predict(x_valid)
valid_pred = (valid_probs > 0.5).astype(int)
np.mean(y_valid == valid_pred)
validation_set_error = 1 - np.mean(y_valid == valid_pred)
print("Validation Set Error:", validation_set_error)

default_data = load_data('Default')
default_train , default_valid = train_test_split (default_data,
test_size = 1000 ,
random_state =0)
default_mm = MS(['balance', 'income'])
x_train = default_mm.fit_transform (default_train)
y_train = default_train['default'].map({'No': 0, 'Yes': 1})
model = sm.OLS(y_train , x_train)
results = model.fit()
x_valid = default_mm.transform(default_valid )
y_valid = default_valid['default'].map({'No': 0, 'Yes': 1})
valid_probs = results.predict(x_valid)
valid_pred = (valid_probs > 0.5).astype(int)
np.mean(y_valid == valid_pred)
validation_set_error = 1 - np.mean(y_valid == valid_pred)
print("Validation Set Error:", validation_set_error)

"""#There was a trend where smaller test sizes than the original half split lead to higher validation set error and when the test size was increased to 7500 (3/4 split) the valid set error decreased.

#Q5(d) - Now consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.
"""

default_data = load_data('Default')
default = pd.DataFrame(default_data)

default['student'] = default['student'].map({'No': 0, 'Yes': 1})

allvars = default.columns.drop (['default'])
design = MS(allvars)
x = design. fit_transform(default)
y = default['default'] == 'Yes'

glm = sm.GLM(y,
             x,
             family=sm. families.Binomial())
results = glm.fit()
results.summary()

default_train , default_valid = train_test_split (default_data,
test_size =5000 ,
random_state =0)

default_mm = MS(['balance', 'income','student'])
x_train = default_mm.fit_transform (default_train)
y_train = default_train['default'].map({'No': 0, 'Yes': 1})
model = sm.OLS(y_train , x_train)
results = model.fit()

x_valid = default_mm.transform(default_valid )
y_valid = default_valid['default'].map({'No': 0, 'Yes': 1})
valid_probs = results.predict(x_valid)
valid_pred = (valid_probs > 0.5).astype(int)
np.mean(y_valid == valid_pred)
validation_set_error = 1 - np.mean(y_valid == valid_pred)
print("Validation Set Error:", validation_set_error)

"""#The set error is nearly identical which means the student variable only changed the outcome slightly.

#Q6 - We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coeffi- cients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the sm.GLM() function. Do not forget to set a random seed before beginning your analysis.

#(a)Using the summarize() and sm.GLM() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.
"""

default_mm = MS(['balance', 'income'])
x = default_mm.fit_transform(default)
y = default['default'].map({'No': 0, 'Yes': 1})

glm = sm.GLM(y, x, family=sm.families.Binomial())
results = glm.fit()

print(results.summary())

income_std_err = results.bse['income']
balance_std_err = results.bse['balance']

print(f"\nStandard Error for Income: {income_std_err}")
print(f"Standard Error for Balance: {balance_std_err}")

"""#(b) Write a function, boot_fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model."""

def boot_fn(data, index):
  default_mm = MS(['balance', 'income'])
  x = default_mm.fit_transform(data.iloc[index])
  y = data.iloc[index]['default'].map({'No': 0, 'Yes': 1})

  glm = sm.GLM(y, x, family=sm.families.Binomial())
  results = glm.fit()
  return results.params['income'], results.params['balance']

boot_fn(default_data, range(len(default_data)))

"""#(c) Following the bootstrap example in the lab, use your boot_fn() function to estimate the standard errors of the logistic regression coefficients for income and balance."""

default_bstrap = default_data.copy()
default_bstrap['Y'] = default_bstrap['default'].map({'No': 0, 'Yes': 1})
def alpha_func (D, idx):
  cov_ = np.cov(D[['income','balance','Y']].loc[idx], rowvar=False)
  return (( cov_ [1 ,1] - cov_[0 ,1]) /
          (cov_ [0 ,0]+ cov_ [1 ,1] -2* cov_ [0 ,1]))

alpha_func(default_bstrap, range(len(default_data)))

rng = np.random. default_rng (0)
alpha_func (default_bstrap , rng.choice (10000, 10000, replace=True))

def boot_SE(func, D, n=None ,B=1000 ,seed =0):
  rng = np.random. default_rng (seed)
  estimates = []
  first_ , second_ = 0, 0
  n = n or D.shape [0]
  for _ in range(B):
    idx = rng.choice(D.index ,
                     n, replace=True)
    income_coef_est, balance_coef_est = func(D, idx)
    estimates.append([income_coef_est, balance_coef_est])
  estimates = np.array(estimates)
  income_se = np.std(estimates[:, 0])
  balance_se = np.std(estimates[:, 1])

  return income_se, balance_se

income_bse, balance_bse = boot_SE(boot_fn, default_data)
print(f"Bootstrap SE for Income: {income_bse}")
print(f"Bootstrap SE for Balance: {balance_bse}")

"""#(d) - Comment on the estimated standard errors obtained using the sm.GLM() function and using the bootstrap."""

income_se_diff = income_std_err - income_bse
balance_se_diff = balance_std_err - balance_bse

print(f"Difference in SE for Income: {income_se_diff}")
print(f"Difference in SE for Balance: {balance_se_diff}")

"""#The standard errors obtained using the sm.GLM() function and the bootstrap method are very similar for both income and balance. The small differences observed can be attributed to the  randomness in the bootstrap procedure. The fact that the standard errors are close provides support for the validity of both approaches. However, the bootstrap method offers a more general approach that can be applied to situations where the standard formulas might not be accurate. Overall, both methods provide similar estimates for the standard errors of the income and balance coefficients, giving us confidence in the precision of our logistic regression model."""